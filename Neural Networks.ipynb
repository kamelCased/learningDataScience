{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "### Outline: \n",
    "\n",
    "#### Theory topics: \n",
    "* Perceptron Model to Neural Networks\n",
    "* Activation Functions\n",
    "* Cost Functions\n",
    "* Feed Forward Networks\n",
    "* Backpropagation\n",
    "\n",
    "#### Coding topics:\n",
    "* Tensorflow 2.0 with Keras Syntax\n",
    "* Neural Networks with Keras \n",
    "    * Feature Engineering\n",
    "    * Classification \n",
    "    * Regression\n",
    "* Exercises for Keras ANN\n",
    "* Tensorboard Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Model\n",
    "\n",
    "`Inputs (x1, x2, ...xn)` --> `Perceptron (f(x))` --> `Output (y)`\n",
    "\n",
    "For a simple summation function:\n",
    "\n",
    "`f(x) = x1 + x2 = y`\n",
    "\n",
    "-------\n",
    "\n",
    "We need the perceptron to adjust some parameter in order to learn. So we add in weights.\n",
    "\n",
    "`Inputs (x1, x2, ...xn)` -- parameters `(w1,w2, ...wn)`--> `Perceptron (f(x))` --> `Output (y)`\n",
    "\n",
    "For a simple summation function:\n",
    "\n",
    "`f(x) = w1x1 + w2x2 + ... wnxn = y`\n",
    "\n",
    "----------\n",
    "\n",
    "What if the input is zero. Then we multiply the weight by 0 and the perceptron will not learn. So we add in a bias term b.\n",
    "\n",
    "`Inputs (x1, x2, ...xn)` -- parameters`(w1,w2, ...wn)` `(b)`--> `Perceptron (f(x))` --> `Output (y)`\n",
    "\n",
    "For a simple summation function:\n",
    "\n",
    "`f(x) = (w1x1 + b) + (w2x2 + b) + ... + (wnxn + b) = y`\n",
    "\n",
    "----------\n",
    "\n",
    "A single perceptron isn't enough to learn complicated systems. We can expand on the idea of a single perceptron to create a multilayer perceptron (neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model\n",
    "\n",
    "* To build a neural network. We can <b>connect layers of perceptrons</b> into a mutlilayer perceptron model.\n",
    "\n",
    "\n",
    "* The outputs of one perceptron layer are then fed as inputs into the next perceptron layer.\n",
    "\n",
    "\n",
    "* This allows the network as a whole to learn about interactions and relationships between features. \n",
    "\n",
    "\n",
    "* The first layer is the <b>input layer</b>. This layer directly receives the data.\n",
    "\n",
    "\n",
    "* The last layer is the <b>output layer</b>. This layer outputs the final predictions.\n",
    "\n",
    "\n",
    "* All the layers in between are called <b>hidden layers</b>. Hidden layers are difficult to intepret due to their high interconnectivity and distance from the known input and output values. \n",
    "\n",
    "\n",
    "* A neural network becomes a <b>deep neural network</b> when it contains two or more hidden layers.\n",
    "    * Network <b>width</b>: How many neurons in a single hidden layer.\n",
    "    * Network <b>depth</b>: How many layers of neurons.\n",
    "    \n",
    "    \n",
    "* A neural network can be used to approximate any convex continuous function. \n",
    "\n",
    "--------\n",
    "\n",
    "For real tasks, we'll want to set constraints on our output values to get useful predictions.\n",
    "   * For classification, it would be very useful to have all the outputs fall between 0 and 1.\n",
    "   * The output value can then represent the probability assignments of each class.\n",
    "\n",
    "Let's explore how to use <b>activation functions</b> to set boundaries on the neuron's output values. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "In general, for each input we have: \n",
    "    ` w*x + b`\n",
    "* `w` tells us how much weight to assign to the incoming input. \n",
    "* `b` is an offset value, making it so that `w*x` needs to overcome a certain threshold before having an effect.\n",
    "* Next we want to set an overall boundary to `z = w*x + b` and then pass it into an activation function `f(z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Function\n",
    "\n",
    "The most simple networks rely on a basic step fucntion that outputs 0 or 1 based on the value of z. However, this function has a very sharp cutoff between 0 and 1 and small changes aren't noticed.\n",
    "\n",
    "#### Sigmoid Function (Logistic Function)\n",
    "\n",
    "The sigmoid function outputs values between 0 and 1 based on the values of z. \n",
    "   * It is more sensitive to small changes and has a smoother transition than the step function. \n",
    "   * Its output can also be used to represent the probability of being 0 or 1.\n",
    "\n",
    "#### Hyperbolic Tangent Function (tanh)\n",
    "\n",
    "tanh outputs values between -1 and 1 instead of 0 and 1.\n",
    "\n",
    "#### Rectified Linear Unit Function (ReLu)\n",
    "\n",
    "relu uses the simple function max(0,z). So for z < 0 it outputs 0 and for z >= 0 it outputs z.\n",
    "   * ReLu has very good performance and deals with the issue of <b>vanishing gradient</b>.\n",
    "   * Recently, ReLu variants (Leaky ReLus, ELUs) solve other issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutliclass Classification Considerations\n",
    "\n",
    "There are 2 main types of multiclass classification problems:\n",
    "* <b>Non-exclusive classes:</b> A data point can be in multiple classes. \n",
    "* <b>Mutually-exclusive classes:</b> A data point can only be in one class. \n",
    "\n",
    "-----\n",
    "\n",
    "#### Organizing mutliple classes: \n",
    "* The easiest way ot organize multiple classes is to have <b>one output node per class</b>.\n",
    "* Then use <b>one-hot encoding</b> to transform data to 0 (not in class) or 1 (in class). \n",
    "\n",
    "Now that we have our data correctly organized all we need to do is choose the correct activation function. \n",
    "\n",
    "-----\n",
    "\n",
    "#### Choosing activation function: \n",
    "* Non-exclusive classification --> sigmoid function \n",
    "    * Each neuron will output a value between 0 and 1 that will represent the probability of the data point being in that class.\n",
    "    * Allows each neuron to output independently of the others.\n",
    "\n",
    "\n",
    "* Mutually-exclusive classification --> softmax function\n",
    "    * Calculates the probabilities distribution of each target class over all the K possible target classes. \n",
    "    * Since all the probabilities sum to 1. softmax chooses the target class with the highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
