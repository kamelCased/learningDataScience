{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "### Outline: \n",
    "\n",
    "#### Theory topics: \n",
    "* Perceptron Model to Neural Networks\n",
    "* Activation Functions\n",
    "* Cost Functions\n",
    "* Feed Forward Networks\n",
    "* Backpropagation\n",
    "\n",
    "#### Coding topics:\n",
    "* Tensorflow 2.0 with Keras Syntax\n",
    "* Neural Networks with Keras \n",
    "    * Feature Engineering\n",
    "    * Classification \n",
    "    * Regression\n",
    "* Exercises for Keras ANN\n",
    "* Tensorboard Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Model\n",
    "\n",
    "`Inputs (x1, x2, ...xn)` --> `Perceptron (f(x))` --> `Output (y)`\n",
    "\n",
    "For a simple summation function:\n",
    "\n",
    "`f(x) = x1 + x2 = y`\n",
    "\n",
    "-------\n",
    "\n",
    "We need the perceptron to adjust some parameter in order to learn. So we add in weights.\n",
    "\n",
    "`Inputs (x1, x2, ...xn)` -- parameters `(w1,w2, ...wn)`--> `Perceptron (f(x))` --> `Output (y)`\n",
    "\n",
    "For a simple summation function:\n",
    "\n",
    "`f(x) = w1x1 + w2x2 + ... wnxn = y`\n",
    "\n",
    "----------\n",
    "\n",
    "What if the input is zero. Then we multiply the weight by 0 and the perceptron will not learn. So we add in a bias term b.\n",
    "\n",
    "`Inputs (x1, x2, ...xn)` -- parameters`(w1,w2, ...wn)` `(b)`--> `Perceptron (f(x))` --> `Output (y)`\n",
    "\n",
    "For a simple summation function:\n",
    "\n",
    "`f(x) = (w1x1 + b) + (w2x2 + b) + ... + (wnxn + b) = y`\n",
    "\n",
    "----------\n",
    "\n",
    "A single perceptron isn't enough to learn complicated systems. We can expand on the idea of a single perceptron to create a multilayer perceptron (neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Model\n",
    "\n",
    "* To build a neural network. We can <b>connect layers of perceptrons</b> into a mutlilayer perceptron model.\n",
    "\n",
    "\n",
    "* The outputs of one perceptron layer are then fed as inputs into the next perceptron layer.\n",
    "\n",
    "\n",
    "* This allows the network as a whole to learn about interactions and relationships between features. \n",
    "\n",
    "\n",
    "* The first layer is the <b>input layer</b>. This layer directly receives the data.\n",
    "\n",
    "\n",
    "* The last layer is the <b>output layer</b>. This layer outputs the final predictions.\n",
    "\n",
    "\n",
    "* All the layers in between are called <b>hidden layers</b>. Hidden layers are difficult to intepret due to their high interconnectivity and distance from the known input and output values. \n",
    "\n",
    "\n",
    "* A neural network becomes a <b>deep neural network</b> when it contains two or more hidden layers.\n",
    "    * Network <b>width</b>: How many neurons in a single hidden layer.\n",
    "    * Network <b>depth</b>: How many layers of neurons.\n",
    "    \n",
    "    \n",
    "* A neural network can be used to approximate any convex continuous function. \n",
    "\n",
    "--------\n",
    "\n",
    "For real tasks, we'll want to set constraints on our output values to get useful predictions.\n",
    "   * For classification, it would be very useful to have all the outputs fall between 0 and 1.\n",
    "   * The output value can then represent the probability assignments of each class.\n",
    "\n",
    "Let's explore how to use <b>activation functions</b> to set boundaries on the neuron's output values. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "In general, for each input we have: \n",
    "    ` w*x + b`\n",
    "* `w` tells us how much weight to assign to the incoming input. \n",
    "* `b` is an offset value, making it so that `w*x` needs to overcome a certain threshold before having an effect.\n",
    "* Next we want to set an overall boundary to `z = w*x + b` and then pass it into an activation function `f(z)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Function\n",
    "\n",
    "The most simple networks rely on a basic step fucntion that outputs 0 or 1 based on the value of z. However, this function has a very sharp cutoff between 0 and 1 and small changes aren't noticed.\n",
    "\n",
    "#### Sigmoid Function (Logistic Function)\n",
    "\n",
    "The sigmoid function outputs values between 0 and 1 based on the values of z. \n",
    "   * It is more sensitive to small changes and has a smoother transition than the step function. \n",
    "   * Its output can also be used to represent the probability of being 0 or 1.\n",
    "\n",
    "#### Hyperbolic Tangent Function (tanh)\n",
    "\n",
    "tanh outputs values between -1 and 1 instead of 0 and 1.\n",
    "\n",
    "#### Rectified Linear Unit Function (ReLu)\n",
    "\n",
    "relu uses the simple function max(0,z). So for z < 0 it outputs 0 and for z >= 0 it outputs z.\n",
    "   * ReLu has very good performance and deals with the issue of <b>vanishing gradient</b>.\n",
    "   * Recently, ReLu variants (Leaky ReLus, ELUs) solve other issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutliclass Classification Considerations\n",
    "\n",
    "There are 2 main types of multiclass classification problems:\n",
    "* <b>Non-exclusive classes:</b> A data point can be in multiple classes. \n",
    "* <b>Mutually-exclusive classes:</b> A data point can only be in one class. \n",
    "\n",
    "-----\n",
    "\n",
    "#### Organizing mutliple classes: \n",
    "* The easiest way ot organize multiple classes is to have <b>one output node per class</b>.\n",
    "* Then use <b>one-hot encoding</b> to transform data to 0 (not in class) or 1 (in class). \n",
    "\n",
    "Now that we have our data correctly organized all we need to do is choose the correct activation function. \n",
    "\n",
    "-----\n",
    "\n",
    "#### Choosing activation function: \n",
    "* Non-exclusive classification --> sigmoid function \n",
    "    * Each neuron will output a value between 0 and 1 that will represent the probability of the data point being in that class.\n",
    "    * Allows each neuron to output independently of the others.\n",
    "\n",
    "\n",
    "* Mutually-exclusive classification --> softmax function\n",
    "    * Calculates the probabilities distribution of each target class over all the K possible target classes. \n",
    "    * Since all the probabilities sum to 1. softmax chooses the target class with the highest probability. \n",
    "    \n",
    "-------\n",
    "Recap to this point:\n",
    "- Neural networks take in inputs, multiply them by weights, add biases to them and pass them into an activation function which at the end of all layers leads to some output. \n",
    "- This output a is the model's estimation. After the model creates this prediction, how do we evaluate it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions\n",
    "\n",
    "* During model training, the cost function C takes the predicted outputs and then <b>compares them to the real target values</b>.\n",
    "    * The cost function must be an average so it can output a single value. \n",
    "    * We can keep track of the loss/cost over training to monitor network performance. \n",
    "    \n",
    "    \n",
    "   \n",
    "* A common cost function is the <b>quadratic cost function</b>.\n",
    "    * We simply calculate the difference between the real value y and the predicted value Ã¿, square the difference, take the sum and divide by 1/2n to average.\n",
    "    * Squaring allows to keep everything positive and punish large errors. \n",
    "  \n",
    "  \n",
    "* For classification, we often use the <b>cross-entropy cost function</b>. \n",
    "    * The cross entropy model assumes that you have a probability distribution for each class. \n",
    "\n",
    "\n",
    "* The network's goal is to figure out which weights w <b>minimize C</b>. We can use a stochastic process to solve for the set of w that leads to the minimal C: Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "* Gradient Descent:\n",
    "    1. <b> Start at a random point</b> based on the initial weights w. \n",
    "    2. Calculate the <b>slope</b> at that point. \n",
    "    3. <b>Move in the downward direction</b> of the slope. \n",
    "    4. Repeat until you reach minimum.\n",
    "    \n",
    "    \n",
    "* We can change the <b>step size (learning rate)</b> to find the minimum: \n",
    "    * Smaller step sizes take longer to find the minimum.\n",
    "    * Larger step sizes are faster but risk overshooting the minimum and having trouble converging.\n",
    "    \n",
    "* We can use <b>adaptive gradient descent</b> to converge better: \n",
    "    * The learning rate doesn't have to be constant.\n",
    "        * Start with larger learning rate and decrease as the slope gets closer to 0.\n",
    "        * 'adam' is a great solver method for stochastic optimization.\n",
    "        \n",
    "------\n",
    "\n",
    "After evaluation with the cost function how do we update the network's weights and biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation \n",
    "\n",
    "The main idea is that we can use the gradient to go back through the network and ajdust our weights and biases to \n",
    "minimize the output of the error vector on the last output layer. \n",
    "\n",
    "\n",
    "For a network with L layers where L is the output layer. \n",
    "   * The notation becomes `L-n` <-- ... <-- `L-1` <-- `L`\n",
    "   * Focusing on the last layer, we have: `z^L = w^L * a^(L-1) + b^L` and `a^L = f(z^L)` and `C = (y - a^L)^2`.\n",
    "   * To understand how sensitive the cost function is to changes in w, we take the partial derivative of `C` with respect to `w^L`.\n",
    "       * Applying the chain rule we get a partial derivative formula with a, z, and C terms with respect to w.\n",
    "   * To understand how sensitive the cost function is to changes in b, we take the partial derivative of `C` with respect to `b^L`.\n",
    "       * Applying the chain rule we get a partial derivative formula with a, z, and C terms with respect to b.\n",
    "   * For networks with multiple neurons per layer we put the neurons into a matrix and use the <b>Hadamard product</b> to do elementwise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
